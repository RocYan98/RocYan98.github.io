import{_ as o,r as s,o as r,c as l,b as a,e,d as t,f as i}from"./app-15703088.js";const c={},h=a("h2",{id:"d3ga-drivable-3d-gaussian-avatars",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#d3ga-drivable-3d-gaussian-avatars","aria-hidden":"true"},"#"),e(" D3GA：Drivable 3D Gaussian Avatars")],-1),d={href:"https://zielon.github.io/d3ga/",target:"_blank",rel:"noopener noreferrer"},g=a("p",null,"arXiv preprint arXiv:2311.08581",-1),p=a("figure",null,[a("img",{src:"https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261143740.png",alt:"Overview",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Overview")],-1),u=a("figure",null,[a("img",{src:"https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261143918.png",alt:"Pipeline",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Pipeline")],-1),_=a("h3",{id:"abstract",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#abstract","aria-hidden":"true"},"#"),e(" Abstract")],-1),m=a("p",null,"我们提出了可驱动 3D 高斯数字人 (D3GA)，这是首个用 3DGS 渲染的人体三维可控模型。目前逼真的数字人在训练过程中需要精确的 3D 注册，在测试过程中需要密集的输入图像，或者两者兼而有之。基于神经辐射场的数字人在远程应用中也往往过于缓慢。本研究利用最近提出的 3DGS 技术，以密集配准的多视角视频作为输入，实时渲染逼真的人体。为了对这些基元进行变形，我们放弃了常用的 LBS 变形方法，而采用了经典的体积变形方法：笼式变形。鉴于它们的尺寸较小，我们用关节角度和关键点来驱动这些变形，这更适合通信应用。在使用相同的训练和测试数据时，我们对九个具有不同体形、衣服和动作的测试者进行了实验，结果比最先进的方法质量更高。",-1),f={href:"http://proceedings.mlr.press/v97/tan19a.html?ref=jina-ai-gmbh.ghost.io",target:"_blank",rel:"noopener noreferrer"},b={href:"https://openaccess.thecvf.com/content_CVPR_2020/html/Kirillov_PointRend_Image_Segmentation_As_Rendering_CVPR_2020_paper.html",target:"_blank",rel:"noopener noreferrer"},v=a("h2",{id:"dynamic-point-field",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#dynamic-point-field","aria-hidden":"true"},"#"),e(" Dynamic Point Field")],-1),y={href:"https://sergeyprokudin.github.io/dpf/",target:"_blank",rel:"noopener noreferrer"},D=i('<p>ICCV 2023</p><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261143606.png" alt="Overview" height="400" tabindex="0" loading="lazy"><figcaption>Overview</figcaption></figure><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261144768.png" alt="Overview" tabindex="0" loading="lazy"><figcaption>Overview</figcaption></figure><h3 id="abstract-1" tabindex="-1"><a class="header-anchor" href="#abstract-1" aria-hidden="true">#</a> Abstract</h3><p>近年来，神经表面重建领域取得了重大进展。在广泛关注体积和隐式方法的同时，一些研究表明，显式图形基元 (如点云) 可以显著降低计算复杂度，同时不影响重建表面的质量。然而，人们较少关注用点云基元对动态表面建模。在这项工作中，我们提出了一种动态点云场模型，该模型结合了显式点云基元的表示优势和隐式形变网络优势，可对非刚性三维表面进行高效建模。通过使用显式表面，我们还可以轻松地将 as-isometric-as-possible 等成熟的约束条件纳入其中。虽然在完全无监督的情况下学习这种变形模型容易出现局部最优，但我们建议同时利用关键点动态等语义信息来指导学习。我们通过一个应用实例来演示我们的模型，即从三维扫描集合中创建一个富有表现力的可动画化的人体。在这里，以前的方法大多依赖于 LBS，这从根本上限制了此类模型在处理长裙等复杂布料外观时的表现力。</p><blockquote><p>这篇文章主要是学习一个动态点云场，可以理解为学习一个基于点云的 PSD，要有 GT 的 mesh 才能学出点云表示的表面。</p></blockquote><h2 id="gaussianavatar-towards-realistic-human-avatar-modeling-from-a-single-video-via-animatable-3d-gaussians" tabindex="-1"><a class="header-anchor" href="#gaussianavatar-towards-realistic-human-avatar-modeling-from-a-single-video-via-animatable-3d-gaussians" aria-hidden="true">#</a> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</h2>',7),x={href:"https://huliangxiao.github.io/GaussianAvatar",target:"_blank",rel:"noopener noreferrer"},k=i('<p>CVPR 2024</p><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202407051530107.png" alt="Overview" tabindex="0" loading="lazy"><figcaption>Overview</figcaption></figure><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202407051531677.png" alt="Pipeline" tabindex="0" loading="lazy"><figcaption>Pipeline</figcaption></figure><h3 id="abstract-2" tabindex="-1"><a class="header-anchor" href="#abstract-2" aria-hidden="true">#</a> Abstract</h3><p>本文介绍的<strong>高斯数字人 (GaussianAvatar)</strong> 是一种高效的方法，可通过单个视频创建具有动态三维外观的逼真数字人。我们首先引入了可动画化的 3D 高斯来显式表示不同姿势和服装的人体。这种显式且可动画化的表现形式可以更高效、更一致地从 2D <strong>流形 (manifold)</strong> 中学习 3D 外观。本文的表示进一步增强了动态属性，以支持 pose-dependent 的外观建模，其中通过动态外观网络和可优化的特征张量可以学习 motion-to-appearance 的映射。此外，通过利用可微 motion condition，本文的方法可以在数字人建模过程中对 motion 和 appearance 进行联合优化，这有助于解决长期存在的单目环境下运动估计不准确的问题。GaussianAvatar 在公开数据集和我们收集的数据集上都得到了验证，证明了它在外观质量和渲染效率方面的卓越表现。</p>',5),A=a("a",{href:"Animatable-Gaussians.thml"},"Animatable Gaussians",-1),P={href:"https://openaccess.thecvf.com/content/ICCV2021/html/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.html",target:"_blank",rel:"noopener noreferrer"},w=a("h2",{id:"ash-animatable-gaussian-splats-for-efficient-and-photoreal-human-rendering",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#ash-animatable-gaussian-splats-for-efficient-and-photoreal-human-rendering","aria-hidden":"true"},"#"),e(" ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering")],-1),z={href:"https://vcai.mpi-inf.mpg.de/projects/ash/",target:"_blank",rel:"noopener noreferrer"},G=a("p",null,"CVPR 2024",-1),S=a("figure",null,[a("img",{src:"https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202407051701353.png",alt:"Overview",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Overview")],-1),C=a("figure",null,[a("img",{src:"https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202407051701447.png",alt:"Pipeline",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Pipeline")],-1),V=a("h2",{id:"abstract-3",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#abstract-3","aria-hidden":"true"},"#"),e(" Abstract")],-1),O=a("p",null,"实时渲染逼真、可控的数字人是计算机视觉和图形学的基石。虽然隐式神经渲染技术的最新进展为数字人带来了前所未有的逼真度，但实时性能大多只在静态场景中得到了验证。为了解决这个问题，本文提出了 ASH，一种可动画化的 3DGS 方法，用于实时逼真地渲染动态人体。我们将穿着衣服的人体参数化为可动画化的 3D 高斯，并将其有效地 splatting 到图像空间中，以生成最终的渲染效果。然而，在 3D 空间中天真地学习高斯参数对计算能力提出了严峻的挑战。相反，我们将高斯附加到可变形的角色模型上，并在 2D 纹理空间中学习它们的参数，这样就可以利用高效的 2D 卷积架构，轻松扩展所需的高斯数量。我们在姿态可控的数字人上对 ASH 与其他竞争方法进行了基准测试，结果表明本文的方法远远优于现有的实时方法，并显示出与离线方法相当甚至更好的效果。",-1),R=a("a",{href:"Animatable-Gaussians.thml"},"Animatable Gaussians",-1),H={href:"https://dl.acm.org/doi/abs/10.1145/3450626.3459749",target:"_blank",rel:"noopener noreferrer"},I=a("strong",null,"对偶四元数蒙皮 (Dual Quaternion skinning)",-1);function q(B,E){const n=s("ExternalLinkIcon");return r(),l("div",null,[h,a("p",null,[a("a",d,[e("项目地址"),t(n)])]),g,p,u,_,m,a("blockquote",null,[a("p",null,[e('整体流程如 Pipeline 所示，主要是想看这篇文章是分割出衣服的 —— "To create a cage per garment, we segment all images of a single time instance using an '),a("a",f,[e("EfficientNet"),t(n)]),e(" backbone with "),a("a",b,[e("PointRend"),t(n)]),e(' refinement."')])]),v,a("p",null,[a("a",y,[e("项目地址"),t(n)])]),D,a("p",null,[a("a",x,[e("项目地址"),t(n)])]),k,a("blockquote",null,[a("p",null,[e("这篇工作的整体思路和 "),A,e(" 很相似，都是用 2D map 来表示 pose，然后直接通过网络学习出高斯的属性。本文的输入是单个视频，不同 pose下每个顶点的位置用 uv 图 (颜色表示具体的坐标) 表示，然后通过动态外观网络 (网络的架构和 "),a("a",P,[e("POP"),t(n)]),e(" 一样) 来学习出高斯的属性。本文还用 motion and appearance 联合优化的方法来缓解 SMPL pose 估计不准的问题。")])]),w,a("p",null,[a("a",z,[e("项目地址"),t(n)])]),G,S,C,V,O,a("blockquote",null,[a("p",null,[e("这篇工作的整体思路和 "),R,e(" 也很相似，输入的也是多视角的视频。首先需要通过 "),a("a",H,[e("Real-time deep dynamic characters"),t(n)]),e(" 生成一个可动画的人体 mesh 模版，根据这个模板可以生成 motion-aware 的 2D 纹理，然后基于 2D 纹理通过两个 2D 卷积神经网络 (几何网络和外观网络) 预测出高斯的属性，最后用"),I,e(" 将人体从标准空间变换到 pose 空间。")])])])}const N=o(c,[["render",q],["__file","Skimming.html.vue"]]);export{N as default};
