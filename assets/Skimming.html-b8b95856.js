import{_ as o,r,o as s,c as l,b as a,e,d as i,f as t}from"./app-9a8474ab.js";const c={},h=a("h2",{id:"d3ga-drivable-3d-gaussian-avatars",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#d3ga-drivable-3d-gaussian-avatars","aria-hidden":"true"},"#"),e(" D3GA：Drivable 3D Gaussian Avatars")],-1),d={href:"https://zielon.github.io/d3ga/",target:"_blank",rel:"noopener noreferrer"},g=a("p",null,"arXiv preprint arXiv:2311.08581",-1),p=a("figure",null,[a("img",{src:"https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261143740.png",alt:"Overview",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Overview")],-1),u=a("figure",null,[a("img",{src:"https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261143918.png",alt:"Pipeline",tabindex:"0",loading:"lazy"}),a("figcaption",null,"Pipeline")],-1),_=a("h3",{id:"abstract",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#abstract","aria-hidden":"true"},"#"),e(" Abstract")],-1),m=a("p",null,"我们提出了可驱动 3D 高斯数字人 (D3GA)，这是首个用 3DGS 渲染的人体三维可控模型。目前逼真的数字人在训练过程中需要精确的 3D 注册，在测试过程中需要密集的输入图像，或者两者兼而有之。基于神经辐射场的数字人在远程应用中也往往过于缓慢。本研究利用最近提出的 3DGS 技术，以密集配准的多视角视频作为输入，实时渲染逼真的人体。为了对这些基元进行变形，我们放弃了常用的 LBS 变形方法，而采用了经典的体积变形方法：笼式变形。鉴于它们的尺寸较小，我们用关节角度和关键点来驱动这些变形，这更适合通信应用。在使用相同的训练和测试数据时，我们对九个具有不同体形、衣服和动作的测试者进行了实验，结果比最先进的方法质量更高。",-1),f={href:"http://proceedings.mlr.press/v97/tan19a.html?ref=jina-ai-gmbh.ghost.io",target:"_blank",rel:"noopener noreferrer"},b={href:"https://openaccess.thecvf.com/content_CVPR_2020/html/Kirillov_PointRend_Image_Segmentation_As_Rendering_CVPR_2020_paper.html",target:"_blank",rel:"noopener noreferrer"},v=a("h2",{id:"dynamic-point-field",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#dynamic-point-field","aria-hidden":"true"},"#"),e(" Dynamic Point Field")],-1),y={href:"https://sergeyprokudin.github.io/dpf/",target:"_blank",rel:"noopener noreferrer"},x=t('<p>ICCV 2023</p><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261143606.png" alt="Overview" height="400" tabindex="0" loading="lazy"><figcaption>Overview</figcaption></figure><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202406261144768.png" alt="Overview" tabindex="0" loading="lazy"><figcaption>Overview</figcaption></figure><h3 id="abstract-1" tabindex="-1"><a class="header-anchor" href="#abstract-1" aria-hidden="true">#</a> Abstract</h3><p>近年来，神经表面重建领域取得了重大进展。在广泛关注体积和隐式方法的同时，一些研究表明，显式图形基元 (如点云) 可以显著降低计算复杂度，同时不影响重建表面的质量。然而，人们较少关注用点云基元对动态表面建模。在这项工作中，我们提出了一种动态点云场模型，该模型结合了显式点云基元的表示优势和隐式形变网络优势，可对非刚性三维表面进行高效建模。通过使用显式表面，我们还可以轻松地将 as-isometric-as-possible 等成熟的约束条件纳入其中。虽然在完全无监督的情况下学习这种变形模型容易出现局部最优，但我们建议同时利用关键点动态等语义信息来指导学习。我们通过一个应用实例来演示我们的模型，即从三维扫描集合中创建一个富有表现力的可动画化的人体。在这里，以前的方法大多依赖于 LBS，这从根本上限制了此类模型在处理长裙等复杂布料外观时的表现力。</p><blockquote><p>这篇文章主要是学习一个动态点云场，可以理解为学习一个基于点云的 PSD，要有 GT 的 mesh 才能学出点云表示的表面。</p></blockquote><h2 id="gaussianavatar-towards-realistic-human-avatar-modeling-from-a-single-video-via-animatable-3d-gaussians" tabindex="-1"><a class="header-anchor" href="#gaussianavatar-towards-realistic-human-avatar-modeling-from-a-single-video-via-animatable-3d-gaussians" aria-hidden="true">#</a> GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</h2>',7),k={href:"https://huliangxiao.github.io/GaussianAvatar",target:"_blank",rel:"noopener noreferrer"},P=t('<p>CVPR 2024</p><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202407051530107.png" alt="Overview" tabindex="0" loading="lazy"><figcaption>Overview</figcaption></figure><figure><img src="https://rocyan.oss-cn-hangzhou.aliyuncs.com/blog/202407051531677.png" alt="Pipeline" tabindex="0" loading="lazy"><figcaption>Pipeline</figcaption></figure><h3 id="abstract-2" tabindex="-1"><a class="header-anchor" href="#abstract-2" aria-hidden="true">#</a> Abstract</h3><p>本文介绍的<strong>高斯数字人 (GaussianAvatar)</strong> 是一种高效的方法，可通过单个视频创建具有动态三维外观的逼真数字人。我们首先引入了可动画化的 3D 高斯来显式表示不同姿势和服装的人体。这种显式且可动画化的表现形式可以更高效、更一致地从 2D <strong>流形 (manifold)</strong> 中学习 3D 外观。本文的表示进一步增强了动态属性，以支持 pose-dependent 的外观建模，其中通过动态外观网络和可优化的特征张量可以学习 motion-to-appearance 的映射。此外，通过利用可微 motion condition，本文的方法可以在数字人建模过程中对 motion 和 appearance 进行联合优化，这有助于解决长期存在的单目环境下运动估计不准确的问题。GaussianAvatar 在公开数据集和我们收集的数据集上都得到了验证，证明了它在外观质量和渲染效率方面的卓越表现。</p>',5),A=a("a",{href:"Animatable-Gaussians.thml"},"Animatable Gaussians",-1),D={href:"https://openaccess.thecvf.com/content/ICCV2021/html/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.html",target:"_blank",rel:"noopener noreferrer"};function w(z,G){const n=r("ExternalLinkIcon");return s(),l("div",null,[h,a("p",null,[a("a",d,[e("项目地址"),i(n)])]),g,p,u,_,m,a("blockquote",null,[a("p",null,[e('整体流程如 Pipeline 所示，主要是想看这篇文章是分割出衣服的 —— "To create a cage per garment, we segment all images of a single time instance using an '),a("a",f,[e("EfficientNet"),i(n)]),e(" backbone with "),a("a",b,[e("PointRend"),i(n)]),e(' refinement."')])]),v,a("p",null,[a("a",y,[e("项目地址"),i(n)])]),x,a("p",null,[a("a",k,[e("项目地址"),i(n)])]),P,a("blockquote",null,[a("p",null,[e("这篇工作的整体思路和 "),A,e(" 很相似，都是用 2D map 来表示 pose，然后直接通过网络学习出高斯的属性。本文的输入是单个视频，不同 pose下每个顶点的位置用 uv 图 (颜色表示具体的坐标) 表示，然后通过动态外观网络 (网络的架构和 "),a("a",D,[e("POP"),i(n)]),e(" 一样) 来学习出高斯的属性。本文还用 motion and appearance 联合优化的方法来缓解 SMPL pose 估计不准的问题。")])])])}const S=o(c,[["render",w],["__file","Skimming.html.vue"]]);export{S as default};
