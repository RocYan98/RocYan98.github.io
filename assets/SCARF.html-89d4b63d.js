const t=JSON.parse(`{"key":"v-0ed2af6c","path":"/posts/paper/SCARF.html","title":"SCARF-论文笔记","lang":"zh-CN","frontmatter":{"date":"2024-03-29T00:00:00.000Z","category":"论文","tag":["Paper","SCARF","Reconstructing","Cloth Simulation"],"title":"SCARF-论文笔记","order":9,"description":"SCARF: Capturing and Animation of Body and Clothing from Monocular Video 项目地址 Siggraph Asia 2022 Abstract 尽管最近的研究表明，从单张图像、视频或一组三维扫描图像中提取穿衣三维人体模型的工作取得了进展，但仍存在一些局限性。大多数方法使用整体表示法对身体和服装进行联合建模，这意味着在虚拟试穿等应用中无法将服装和身体分开。其他方法则将人体和服装分开建模，但需要从三维/四维扫描仪或物理模拟中获得的大量三维穿衣人体 mesh 进行训练。我们的见解是，身体和服装有不同的建模要求。基于 mesh 的参数化三维模型可以很好地表示人体，而隐式表示法和神经辐射场则更适合捕捉服装中的各种形状和外观。基于这一观点，我们提出了 SCARF (Segmented Clothed Avatar Radiance Field)，这是一种将基于 mesh 的人体与神经辐射场相结合的混合模型。将网格与可微分光栅化器相结合集成到体渲染中，使我们能够直接从单目视频中优化 SCARF，而无需任何 3D 监督。混合建模使 SCARF 能够：(i) 通过改变身体姿势 (包括手部衔接和面部表情) 为穿着衣服的 avatar 制作动画；(ii) 合成 avatar 的新视图；(iii) 在虚拟试穿应用中实现 avatar 之间的服装转移。我们证明，与现有方法相比，SCARF 重建的服装具有更高的视觉质量，服装会随着身体姿势和体形的变化而变形，而且服装可以在不同的 avatar 之间成功转移。","head":[["meta",{"property":"og:url","content":"https://rocyan.top/posts/paper/SCARF.html"}],["meta",{"property":"og:site_name","content":"Roc Yan's Blog"}],["meta",{"property":"og:title","content":"SCARF-论文笔记"}],["meta",{"property":"og:description","content":"SCARF: Capturing and Animation of Body and Clothing from Monocular Video 项目地址 Siggraph Asia 2022 Abstract 尽管最近的研究表明，从单张图像、视频或一组三维扫描图像中提取穿衣三维人体模型的工作取得了进展，但仍存在一些局限性。大多数方法使用整体表示法对身体和服装进行联合建模，这意味着在虚拟试穿等应用中无法将服装和身体分开。其他方法则将人体和服装分开建模，但需要从三维/四维扫描仪或物理模拟中获得的大量三维穿衣人体 mesh 进行训练。我们的见解是，身体和服装有不同的建模要求。基于 mesh 的参数化三维模型可以很好地表示人体，而隐式表示法和神经辐射场则更适合捕捉服装中的各种形状和外观。基于这一观点，我们提出了 SCARF (Segmented Clothed Avatar Radiance Field)，这是一种将基于 mesh 的人体与神经辐射场相结合的混合模型。将网格与可微分光栅化器相结合集成到体渲染中，使我们能够直接从单目视频中优化 SCARF，而无需任何 3D 监督。混合建模使 SCARF 能够：(i) 通过改变身体姿势 (包括手部衔接和面部表情) 为穿着衣服的 avatar 制作动画；(ii) 合成 avatar 的新视图；(iii) 在虚拟试穿应用中实现 avatar 之间的服装转移。我们证明，与现有方法相比，SCARF 重建的服装具有更高的视觉质量，服装会随着身体姿势和体形的变化而变形，而且服装可以在不同的 avatar 之间成功转移。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-04-18T07:53:33.000Z"}],["meta",{"property":"article:author","content":"Roc Yan"}],["meta",{"property":"article:tag","content":"Paper"}],["meta",{"property":"article:tag","content":"SCARF"}],["meta",{"property":"article:tag","content":"Reconstructing"}],["meta",{"property":"article:tag","content":"Cloth Simulation"}],["meta",{"property":"article:published_time","content":"2024-03-29T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-04-18T07:53:33.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"SCARF-论文笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-03-29T00:00:00.000Z\\",\\"dateModified\\":\\"2024-04-18T07:53:33.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Roc Yan\\",\\"url\\":\\"https://rocyan.top\\",\\"email\\":\\"qpyan23@m.fudan.edu.cn\\"}]}"]]},"headers":[{"level":2,"title":"SCARF: Capturing and Animation of Body and Clothing from Monocular Video","slug":"scarf-capturing-and-animation-of-body-and-clothing-from-monocular-video","link":"#scarf-capturing-and-animation-of-body-and-clothing-from-monocular-video","children":[]},{"level":2,"title":"Abstract","slug":"abstract","link":"#abstract","children":[]},{"level":2,"title":"Introduction","slug":"introduction","link":"#introduction","children":[]},{"level":2,"title":"Methods","slug":"methods","link":"#methods","children":[{"level":3,"title":"混合表示 (Hybrid Representation)","slug":"混合表示-hybrid-representation","link":"#混合表示-hybrid-representation","children":[]},{"level":3,"title":"标准化 (Canonicalization)","slug":"标准化-canonicalization","link":"#标准化-canonicalization","children":[]},{"level":3,"title":"损失函数","slug":"损失函数","link":"#损失函数","children":[]}]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"git":{"createdTime":1712147254000,"updatedTime":1713426813000,"contributors":[{"name":"Yan","email":"rocyan98@gmail.com","commits":3},{"name":"rocyan","email":"rocyan98@gmail.com","commits":2}]},"readingTime":{"minutes":6.48,"words":1945},"filePathRelative":"posts/paper/SCARF.md","localizedDate":"2024年3月29日","excerpt":"<h2> SCARF: Capturing and Animation of Body and Clothing from Monocular Video</h2>\\n<p><a href=\\"https://yfeng95.github.io/scarf/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">项目地址</a></p>\\n<p>Siggraph Asia 2022</p>\\n<h2> Abstract</h2>\\n<p>尽管最近的研究表明，从单张图像、视频或一组三维扫描图像中提取穿衣三维人体模型的工作取得了进展，但仍存在一些局限性。大多数方法使用整体表示法对身体和服装进行联合建模，这意味着在虚拟试穿等应用中无法将服装和身体分开。其他方法则将人体和服装分开建模，但需要从三维/四维扫描仪或物理模拟中获得的大量三维穿衣人体 mesh 进行训练。我们的见解是，身体和服装有不同的建模要求。基于 mesh 的参数化三维模型可以很好地表示人体，而隐式表示法和神经辐射场则更适合捕捉服装中的各种形状和外观。基于这一观点，我们提出了 SCARF (Segmented Clothed Avatar Radiance Field)，这是一种将基于 mesh 的人体与神经辐射场相结合的混合模型。将网格与可微分光栅化器相结合集成到体渲染中，使我们能够直接从单目视频中优化 SCARF，而无需任何 3D 监督。混合建模使 SCARF 能够：(i) 通过改变身体姿势 (包括手部衔接和面部表情) 为穿着衣服的 avatar 制作动画；(ii) 合成 avatar 的新视图；(iii) 在虚拟试穿应用中实现 avatar 之间的服装转移。我们证明，与现有方法相比，SCARF 重建的服装具有更高的视觉质量，服装会随着身体姿势和体形的变化而变形，而且服装可以在不同的 avatar 之间成功转移。</p>","autoDesc":true}`);export{t as data};
